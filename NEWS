Changes in version 0.9.1.9000

 * Fix bug in utf8_normalize when the input contains a backslash (\)


Changes in version 0.9.1

 * Fix buffer overrun for case folding some Greek letters.

 * Fix memory leak in read_ndjson.

 * Fix memory leak in JSON object deserialization.

 * Fix memory leak in term_stats.

 * Rename 'wnaffect' to 'affect_wordnet'.


Changes in version 0.9.0 (2017-07-19)

 * Add new vignette, "Introduction to corpus" vignette.

 * Add `random` argument to `text_locate` for random order.

 * Change format.corpus_frame to use elastic column widths for text.

 * Allow `rows = -1` for print.corpus_frame to print all rows.

 * Add corpus(), as_corpus(), is_corpus() functions.

 * Make text_split() split into evenly-sized blocks, with the 'size'
   argument specifying the maximum block size.

 * Following *quanteda*, add "will" to the English stop word list.

 * Add special handling for hyphens so that, for example, "world-wide"
   is a single token (but "-world-wide-" is three tokens).

 * Merged 'url' and 'symbol' word categories.  Removed 'other' word
   category (ignore these characters).

 * Renamed term_counts() to term_stats().

 * Added text_stats() function.

 * Removed deprecated functions token_filter() and sentence_filter().

 * Removed 'term' column from text_locate() output.

 * Added text_match() function to return matching terms as a factor.

 * Change stemmer so that it only modifies tokens of kind "letter",
   preserving "number", "symbol", "url", etc.

 * Removed 'map_compat' option from text_filter(); use utf8_normalize()
   instead if you need to apply compatibility maps.

 * Implemented text subset assignment operators `[<-` and `[[<-`.

 * Switched to more efficient c.corpus_text() function.

 * Added utf8_normalize() function for translating to NFC normal form,
   applying case and compatibility maps.

 * Make text_locate() return "text" column as a factor.

 * Constrain text names() to be unique, non-missing.

 * Added "names" argument to as_text for overriding default names.

 * Added text_sub() for getting token sub-sequences.

 * Added text_length() for text length, including NA tokens.

 * Added checks for underflow in read_ndjson() double deserialization.

 * Fixed bug in `text_filter<-` where assignment did not make a deep copy
   of the object.

 * Fixed bug in utf8_format, utf8_print, utf8_width where internal
   double quotes were not escaped.

 * Fixed rchk, UBSAN warnings.


Changes in version 0.8.0 (2017-07-18)

 * Added text_filter() generic. Deprecate token_filter() and
   sentence_filter().

 * Added text_filter()<- setter for text vectors.

 * Use a text's text_filter() attribute as a default in all text_*
   functions expecting a filter argument.

 * Handle @mentions, #hashtags, and URLs in word tokenizer.

 * Added the _Federalist Papers_ dataset ("federalist").

 * term_counts() now reports the "support" for each term (the number of
   texts containing the term), and has options for restricting output by
   the minimum and maximum support.

 * Added new class "corpus_frame" to support better printing of
   data frame objects: left-align text data, truncate output to screen
   width, display emoji on Mac OS. Use this class for all data frame
   return values.

 * Added functions for validating and converting to UTF-8: as_utf8(),
   utf8_valid().

 * Added functions for formatting and printing utf8 text: utf8_encode(),
   utf8_format(), utf8_print(), utf8_valid(), and utf8_width().

 * Added a "unicode" vignette.

 * Converted the "chinese" demo to a vignette. Thanks to Will Lowe for
   contributing.

 * Make text_split() and term_frame() return parent text as a factor.

 * Remove stringsAsFactors option from read_ndjson(); deserialize all
   JSON string fields as character by default.

 * read_ndjson() de-serializes zero-length arrays as integer(), logical(),
   etc. instead of as NULL.

 * Fixed a bug where read_ndjson() would de-serialize a boolean "null"
   as FALSE instead of NA.

 * Allow user interrupts (control-c) in all long-running C computations.


Changes in version 0.7.0 (2017-06-22)

 * Add text_locate(), for searching for specific terms in text,
   reporting the contexts of the occurrences ("Key words in context").

 * Add text_count() and text_detect() for counting term occurrences or
   checking for a term's existence in a text.

 * Add text_types() and text_ntype() for returning the unique types in
   a text, or counting types.

 * Rename tokens() to text_tokens() for consistency; add text_ntoken().

 * Add text_nsentence() for counting sentences.

 * Add term_frame(), reporting term frequencies as a data frame with
   columns "text", "term", and "count".

 * Add transpose argument to term_matrix().

 * Add new version of format.corpus_text() that is faster and aware
   of character widths, in particular, Emoji and East Asian character
   widths.

 * Rename term_counts() "min" and "max" arguments to "min_count" and
   "max_count".

 * Normalize token filter combine, drop, drop_except, stem_except arguments,
   to allow passing cased versions of these arguments.

 * Set combine = abbreviations("english") by default.

 * Fixed bug where "u.s" (a unigram) stems to "u.s" (a bigram), and then
   causes for term_matrix() select.  Thanks to Dmitriy Selivanov for
   reporting: https://github.com/patperry/r-corpus/issues/3 .


Changes in version 0.6.0 (2017-06-06)

 * Rename text_filter() to token_filter().

 * Add "ngrams" options for term_counts() and term_matrix().

 * Add term_counts() "min" and "max" options for excluding terms with
   counts below or above specified limits.

 * Add term_counts() "limit" option to limit the number of reported terms.

 * Add term_counts() "types" option for reporting the types that make up
   a term.

 * Remove "select" argument from token_filter(), but add "select" to
   term_matrix() arguments.

 * Replace sentences() function with text_split(), which has options for
   breaking into multi-sentence blocks or multi-token blocks.

 * Add sentence break suppressions (special handling for abbreviations);
   the default behavior for text_split(, "sentences") is to use a set of
   English abbreviations as suppressions.

 * Add option to treat CR and LF like spaces when determining sentence
   boundaries; this is now the default.

 * Add abbreviations() function with abbreviation lists for English,
   French, German, Italian, Portuguese, and Russian (from the Unicode
   Common Locale Data Repository).

 * Add more refined control over token_filter() drop cateogries:
   merged "kana", and "ideo" into "letter"; split off "punct", "mark",
   and "other" from "symbol".

 * Remove "remove_control", "map_dash", and "remove_space" type
   normalization options from text_filter().

 * Remove "ignore_empty" token filter option.


Changes in version 0.5.1 (2017-05-25)

 * Rename "text" class to "corpus_text" to avoid name classes with grid.
   Thanks to Jeroen Ooms for reporting:
   https://github.com/patperry/corpus/issues/1

 * Rename "jsondata" to "corpus_json" for consistency.

 * Fix bug in read_ndjson() for reading factors with missing values.


Changes in version 0.5.0 (2017-05-23)

 * Add term_counts() function to tabulate term frequencies.

 * Add term_matrix() function to compute a term frequency matrix.

 * Add text_filter() option ("stem_except") to exempt specific terms from
   stemming.

 * Add text_filter() option ("drop") to drop specific terms, along with
   option ("drop_except") to exempt specific terms from dropping.

 * Add text_filter() option ("combine") to combine multi-word phrases like
   "new york city" into a single term.

 * Add text_filter() option ("select") to select specific terms (excluding
   all words that are not on this list).

 * Rename text_filter() options "fold_case", "fold_dash", "fold_quote"
   to "map_case", "map_dash", "map_quote".

 * Add stopwords() function.

 * Make read_ndjson() decode JSON strings as character or factor (according
   to whether "stringsAsFactors" is TRUE) except for fields named "text",
   which get decoded as text objects.


Changes in version 0.4.0 (2017-05-16)

 * Allow read_ndjson() to read from connections, not just files, by
   reading the file contents into memory first. Use this by default
   instead of memory mapping.

 * Rename text_filter() option "drop_empty" to "ignore_empty".

 * Add text_filter() options "drop_symbol", "drop_number", "drop_letter",
   "drop_kana", and "drop_ideo"; these options replace the matched tokens
   with NA.

 * Fix internal function namespace clashes on Linux and other similar
   platforms.


Changes in version 0.3.0 (2017-05-04)

 * Support for serializing dataset and text objects via readRDS() and
   other native routines.  Unfortunately, this support doesn't come for
   free, and the objects take a little bit more memory.

 * More convenient interface for accessing JSON arrays.

 * Rename as.text()/is.text() to as_text()/is_text(); make as_text()
   retain names, work on S3 objects.

 * Add support for stemming via the Snowball library.

 * Rename read_json() to read_ndjson() to not clash with jsonlite.

 * Rename "dataset" type to "jsondata".

 * Make read_ndjson() return a data frame by default, not a "jsondata"
   object.


Changes in version 0.2.0 (2017-04-15)

 * First CRAN release.

 * Added Windows support.

 * Added support for setting names on text objects.

 * Added documentation.


Changes in version 0.1.0 (2017-04-11)

 * First milestone, with support for JSON decoding, text segmentation,
   and text normalization.
