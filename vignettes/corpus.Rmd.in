---
title: "Introduction to corpus"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to corpus}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(comment = "", fig.path = "corpus-", fig.retina = TRUE)
options(width = 100)
```

Overview
--------

This vignette demonstrates the functionality provided by the *corpus* R
package. The running example throughout is an analysis of the text of L. Frank
Baum's novel, *The Wonderful Wizard of Oz*.


Setup
-----

We load the *corpus* package, set the color palette, and set the random number
generator seed.  We will not use any external packages in this vignette.

```{r}
library("corpus")

# colors from RColorBrewer::brewer.pal(6, "Set1")
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00", "#FFFF33"))

# ensure consistent runs
set.seed(0)
```


Data preparation
----------------

The *The Wonderful Wizard of Oz* is available as
[Project Gutenberg EBook #55][gutenberg-oz]. We first download the text and
strip off the Project Gutenberg header and footer.

```{r}
url <- "http://www.gutenberg.org/cache/epub/55/pg55.txt"
raw <- readLines(url, encoding = "UTF-8")

# the text starts after the Project Gutenberg header...
start <- grep("^\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK", raw) + 1

# ...end ends at the Project Gutenberg footer.
stop <- grep("^End of Project Gutenberg", raw) - 1

lines <- raw[start:stop]
```

The novel starts with front matter: a title page, table of contents,
introduction, and half title page. Then, a series of chapters follow.
We group the lines by section.

```{r}
# the front matter ends at the half title page
half_title <- grep("^THE WONDERFUL WIZARD OF OZ", lines)

# chapters start with "1.", "2.", etc...
chapter <- grep("^[[:space:]]*[[:digit:]]+\\.", lines)

# ... and appear after the half title page
chapter <- chapter[chapter > half_title]

# get the section texts (including the front matter)
start <- c(1, chapter + 1) # + 1 to skip title
end <- c(chapter - 1, length(lines))
text <- mapply(function(s, e) paste(lines[s:e], collapse = "\n"), start, end)

# trim leading and trailing white space
text <- trimws(text)

# discard the front matter
text <- text[-1]

# get the section titles, removing the prefix ("1.", "2.", etc.)
title <- sub("^[[:space:]]*[[:digit:]]+[.][[:space:]]*", "", lines[chapter])
title <- trimws(title)
```


Corpus object
-------------

Now that we have obtained our raw data, we put everything together into a
corpus data frame object, constructed via the `corpus_frame()` function:

```{r}
data <- corpus_frame(title, text)

# set the row names; not necessary but makes results easier to read
rownames(data) <- sprintf("ch%02d", seq_along(chapter))
```

The `corpus_frame()` function behaves similarly to the `data.frame` function,
but expects one of the columns to be named `"text"`. Note that we do not need
to specify `stringsAsFactors = FALSE` when creating a corpus data frame
object.  As an alternative to using the `corpus_frame()` function, we can
construct a data frame using some other method (e.g., `read.csv` or
`read_ndjson`) and use the `as_corpus_frame()` function.


A corpus data frame object is just a data frame with a column named "text" of
type `"corpus_text"`. When using the *corpus* library, it is not strictly
necessary to use corpus data frame objects as inputs; most functions will
accept with character vectors, ordinary data frames, *quanteda* corpus
objects, and *tm* Corpus objects.. Using a native corpus object gives better
printing behavior and allows setting a `text_filter` attribute to override the
default text preprocessing.

```{r}
print(data) # better output than printing a data frame, cuts off after 20 rows
print(data, 5) # cuts off after 5 rows
print(data, -1) # prints all rows
```


Tokenization
------------

Text in *corpus* is represented as a sequence of tokens, each taking a value
in a set of types. We can see the tokens for one or more elements using
the `text_tokens` function:

```{r}
text_tokens(data["ch24",]) # Chapter 24's tokens
```

The default behavior is to normalize tokens by changing the cases of the
letters to lower case. A `text_filter` object controls the rules for
segmentation and normalization. We can inspect the text filter:

```{r}
text_filter(data)
```

We can change the text filter properties:

```{r}
text_filter(data)$map_case <- FALSE
text_filter(data)$drop_punct <- TRUE
text_tokens(data["ch24",])
```

To restore the defaults, set the text filter to `NULL`:

```{r}
text_filter(data) <- NULL
```

In addition to mapping case and quotes (the defaults), I'm going to drop
punctuation.

```{r}
text_filter(data) <- text_filter(drop_punct = TRUE)
```

The tokenizer allows for precise controlling over token dropping and token
stemming. It also allows combining two or more words into a single token as
in the following example:

```{r}
text_tokens("I live in New York City, New York",
            combine = c("new york", "new york city"))
```

This example using the optional second argument to `text_tokens` to override
the first argument's default text filter.  Here, instances of "new york" and
"new york city" get replaced by single tokens, with the longest match taking
precedence. See the documentation for `text_tokens` describes the full
tokenization process.


Texts as sequences
------------------

The mental model of the *corpus* package is that a text is s sequence of
tokens, some of which are dropped (`NA`). Every object has a `text_filter()`
property defining its token boundaries. The default token filter transforms
the text to Unicode composed normal form (NFC), applies Unicode case folding,
and maps curly quotes to straight quotes. Text objects, created with
`as_corpus_text` or `as_corpus` can have custom text filters. You cannot set
the text filter for a character vector. However, all *corpus* text functions
accept a `filter` argument to override the input object's text filter (this is
demonstrated in the "New York City" example in the previous section).


To find out the length of a text, as measured in dropped and non-dropped
tokens, use the `text_length` function.


```{r}
text_tokens("One, two, three!", filter = text_filter(drop_punct = TRUE))
text_length("One, two, three!", filter = text_filter(drop_punct = TRUE))
```

You can set subsequences of consecutive tokens using the `text_sub` function.
This function accepts two arguments specifying the start and then end token
position. The following example extracts the subsequences from
positions 2 to 4:

```{r}
text_sub(c("One, two, three!", "4 5 6 7 8 9 10"), 2, 4,
         filter = text_filter(drop_punct = TRUE))
```

Negative indices count from the end of the sequence, with `-1` denoting the
last token.

```{r}
# last 2 tokens
text_sub(c("One, two, three!", "4 5 6 7 8 9 10"), -2, -1,
         filter = text_filter(drop_punct = TRUE))
```

Note that `text_length` and `text_sub` count both dropped and non-dropped
tokens.


Here's how to get the last 10 tokens in each chapter:

```{r}
text_sub(data, -10)
```

In this example, we do not specify the ending position, so it defaults to `-1`.



Text statistics
---------------

### Token, type, and sentence counts

The `text_ntoken`, `text_ntype`, and `text_nsentence` functions return the
numbers of non-dropped tokens, unique types, and sentences, respectively, in a set of
texts. We can use these functions to get an overview of the section lengths
and lexical diversities.

```{r}
text_ntoken(data)
text_ntype(data)
text_nsentence(data)
```

The `text_stats` function computes all three counts and presents the results
in a data frame:

```{r}
stats <- text_stats(data)
print(stats, -1) # print all rows instead of truncating at 20
```

We can see that the last chapter is the shortest, with 74 tokens, 56 unique
types, and 8 sentences. Chapter 12 is the longest.


### Application: Testing Heaps' law

[Heaps' law][heaps-law] says that the logarithm of the number of unique
types is a linear function of the number of tokens. We can test this law
formally with a regression analysis.

In this analysis, we will exclude the last chapter (Chapter 24), because it is
much shorter than the others and has a disproportionate influence on the fit.

```{r}
subset <- row.names(stats) != "ch24"
model <- lm(log(types) ~ log(tokens), stats, subset)
summary(model)
```

We can also inspect the relation visually
```{r heapslaw, fig.width = 12, fig.cap = "Heaps' Law"}
par(mfrow = c(1, 2))
plot(log(types) ~ log(tokens), stats, col = 2, subset = subset)
abline(model, col = 1, lty = 2)

plot(log(stats$tokens[subset]), rstandard(model), col = 2,
     xlab = "log(tokens)")
abline(h = 0, col = 1, lty = 2)

outlier <- abs(rstandard(model)) > 2
text(log(stats$tokens)[subset][outlier], rstandard(model)[outlier],
     row.names(stats)[subset][outlier], cex = 0.75, adj = c(-0.25, 0.5),
     col = 2)
```

The analysis tells us that Heap's law accurately characterizes the lexical
diversity (type-to-token ratio) for the main chapters in *The Wizard of Oz*.
The number of unique types grows roughly as the number of tokens raised to the
power `0.6`.


The one chapter with an unusually low lexical diversity is Chapter 16. This
chapter contains mostly dialogue between Oz and Dorothy's simple-minded
companions (the Scarecrow, Tin Woodman, and Lion).


Term statistics
---------------

### Counts and prevalence

We get term statistics using the `term_stats` function:

```{r}
term_stats(data)
```

This returns a data frame with each row giving the count and support for each
term. The "count" is the total number of occurrences of the term in the
corpus. The "support" is the number of texts containing the term. In the
output above, we can see that "the" is the most common term, appearing 2922
times total in all 24 chapters. The pronoun "her" is the 20th most common
term, appearing in all but one chapter.


The most common words are English function words, commonly known as "stop"
words. We can exclude these terms from the tally using the `subset` argument.

```{r}
term_stats(data, subset = !term %in% stopwords_en)
```

The character names "dorothy", "toto", and "scarecrow" show up at the top of
the list of the most common terms.


### Higher-order n-grams

Beyond searching for single-type terms, we can also search for multi-type
terms ("n-grams").

```{r}
term_stats(data, ngrams = 5)
```

The `types` argument allows us to request the component types in the result:

```{r}
term_stats(data, ngrams = 3, types = TRUE)
```

Here are the most common 2-, 3-grams starting with "dorothy", where the second
type is not a function word
```{r}
term_stats(data, ngrams = 2:3, types = TRUE,
           subset = type1 == "dorothy" & !type2 %in% stopwords_en)
```


Searching for terms
-------------------

Now that we have identified common terms, we might be interested in seeing
where they appear. For this, we use the `text_locate` function.

Here are all instances of the term "dorothy looked":

```{r}
text_locate(data, "dorothy looked")
```

Note that we match against the type of the token, not the raw token itself, so
we are able to detect capitalized "Dorothy". This is especially useful when we
want to search for a stemmed token. Here are all instances of tokens that
stem to "scream":

```{r}
text_locate(data, "scream", stemmer = "english")
```

If we would like, we can search for multiple phrases at the same time:

```{r}
text_locate(data, c("wicked witch", "toto", "oz"))
```

We can also request that the results be returned in random order, using the
`text_sample()` function. This function takes the results from `text_locate()`
and randomly orders the rows; this is useful for inspecting a random sample of
the matches:

```{r}
text_sample(data, c("wicked witch", "toto", "oz"))
```

Other functions allow counting term occurrences, testing for whether a term
appears in a text, and getting the subset of texts containing a term:

```{r}
text_count(data, "the great oz")
text_detect(data, "the great oz")
text_subset(data, "the great oz")
```


Segmenting text
---------------

### Sentences and blocks of tokens

*Corpus* can split text into blocks of sentences or tokens using the
`text_split` function. By default, this function splits into sentences. Here,
for example, are the last 10 sentences in the book:

```{r}
tail(text_split(data), 10)
```

The result of `text_split` is a data frame, with one row for each segment
identifying the parent text (as a factor), the index of the segment in the
parent text (an integer), and the segment text.


The second argument to `text_split` specifies, the units, "sentences" or
"tokens". The third argument specifies the maximum segment size, defaulting
to one. Each text gets divided into approximately equal-sized segments, with
no segment being larger than the specified size.


Here is an example of splitting two texts into segments of size at most four
tokens.

```{r}
text_split(c("the wonderful wizard of oz", paste(LETTERS, collapse = " ")),
           "tokens", 4)
```

### Application: Witch tracking

We can combine `text_split` with `text_count` to measure the occurrences rates
for the term "witch" over the course of the novel.  Here, the chunks have
varying sizes, so we look at the rates rather than the raw counts.


```{r witch-occurrences, fig.width = 12, fig.cap = "'witch' Occurences"}
chunks <- text_split(data, "tokens", 500)
size <- text_ntoken(chunks)

unit <- 1000 # rate per 1000 tokens
count <- text_count(chunks, "witch")
rate <-  count / size * unit

i <- seq_along(rate)
plot(i, rate, type = "l", xlab = "Segment",
     ylab = "Rate \u00d7 1000",
     main = paste(dQuote("witch"), "Occurrences"), col = 2)
points(i, rate, pch = 16, cex = 0.5, col = 2)
```

We can see Dorothy's house landing on the Wicked Witch of the East in the and
the subsequent fallout in the beginning of the novel. Around segment 40, we
see the events surrounding Dorothy's battle with the Wicked Witch of the West.
At the end of the novel, we see the Good Witch of the South appearing to help
Dorothy get home.


Term frequency matrix
---------------------

Many downstream text analysis tasks require tabulating a matrix of text-term
occurrence counts. We can get such a matrix using the `term_matrix` function:

```{r}
x <- term_matrix(data)
dim(x)
```

This function returns a sparse matrix object from the *Matrix* package. In the
default usage, the rows of the matrix correspond to texts, and the columns
correspond to terms. For a "term-by-document" matrix, you can use the
`transpose` option:

```{r}
xt <- term_matrix(data, transpose = TRUE)
```

You can include n-grams in the result if you would like:
```{r}
x3 <- term_matrix(data, ngrams = 1:3) # 1-, 2-, and 3-grams
```

Or, you can specify the columns to include in the matrix

```{r}
(x <- term_matrix(data, select = c("dorothy", "toto", "wicked witch", "the great oz")))
```

The columns of `x` will be in the same order as specified by the `select`
argument. Note that we can request higher-order n-grams.


Emotion lexicon
---------------

*Corpus* provides a lexicon of terms connoting emotional affect, the
[WordNet Affect Lexicon][wn-affect].

```{r}
affect_wordnet
```

This lexicon classifies a large set of terms correlated with emotional affect
into four main categories: "Positive", "Negative", "Ambiguous", and "Neutral",
and a variety of sub-categories. Here is a summary:

```{r}
summary(affect_wordnet)
```

Here are the term counts broken down by category:

```{r}
with(affect_wordnet, table(category, emotion))
```

Terms can appear in multiple categories, or with multiple parts of speech.

```{r}
# some duplicate terms
subset(affect_wordnet, term %in% c("caring", "chill", "hopeful"))
```

The term "chill", for example, is listed as denoting both positive calmness
and negative fear, among other emotional affects.


Application: Emotion in _The Wizard of Oz_
------------------------------------------

### Overview

For our final application, we will track emotion word usage over the course of
_The Wizard of Oz_. We will do this by segmenting the novel into small chunks,
and then measure the occurrence rates of emotion words in these chunks.


### Lexicon

We will first need a lexicon of emotion words. We will take as a starting
point the WordNet-Affect lexicon, but we will remove "Neutral" emotion words.

```{r}
affect <- subset(affect_wordnet, emotion != "Neutral")
affect$emotion <- droplevels(affect$emotion) # drop the unused "Neutral" level
affect$category <- droplevels(affect$category) # drop unused categories
```

Rather than blindly applying the lexicon, we first check to see what the most
common emotion terms are.

```{r}
term_stats(data, subset = term %in% affect$term)
```

A few terms jump out as unusual: "yellow" is probably for the yellow brick
road; "down" and "near" probably do not evoke emotions.  We can inspect the
usages of the most common terms using the `text_locate` function, which shows
these terms in context.

```{r}
text_sample(data, "down")
```

Here, we use the `text_sample()` instead of `text_locate()` to return the
matches in random order.  Since we are only looking at a subset of the
matches, we use this option to ensure that we don't make conclusions about
these words using a biased sample.  Using `text_locate()`, we would would only
see the matches at the beginning of the novel.

It looks like "down" is mostly used as a preposition, not an emotion. We will
exclude it form the lexicon.

```{r}
text_sample(data, "good")
```

"Good" seems to be an appropriate emotion work, evoking positive affection or
love. We will keep it in the lexicon.

```{r}
text_sample(data, "heart")
```

"Heart" is mostly used as an object (noun), not an emotion meaning compassion.
The Tin Woodman's search for a heart is a central plot of the novel, so it is
not surprising that the term shows up frequently. We can look for
co-occurrences of "heart" with "woodman":

```{r}
loc <- text_locate(data, "heart")
before <- text_detect(text_sub(loc$before, -25, -1), "woodman")
after <- text_detect(text_sub(loc$after, 1, 25), "woodman")
summary(before | after)
```

"Woodman" appears within 25 tokens of "heart" in in 45 of the 67 contexts
where the latter word appears.

The decision of whether to include or exclude "heart" is a difficult judgment
call. Most of the time it appears, it describes an object, not an emotion.
Still, that object does have an emotional association.  I'm deciding to
include "heart", but this is not a clear-cut decision.

We can also inspect the first token after each appearance of "yellow":

```{r}
term_stats(text_sub(text_locate(data, "yellow")$after, 1, 1))
```

Over half the time, "yellow" prefaces "brick" or "bricks", and otherwise it
describes objects. It does not describe or evoke emotion, and we should
exclude it from the lexicon.

Similar analysis not shown here indicates that "great" is mostly used to
describe size, not positive enthusiasm; "like" is often used to mean "similar
to", not "affection for"; "blue" is mostly used as a color, not an emotion.

All of this analysis shows that we should probably exclude some of the common
terms from the lexicon.

```{r}
affect <- subset(affect, !term %in% c("down", "great", "like", "yellow", "near", "low", "blue"))
```

### Term emotion matrix

Now that we have a lexicon, our plan is to segment the text into smaller
chunks and then compute the emotion occurrence rates in each chunk, broken
down by category ("Positive", "Negative", or "Ambiguous").

To facilitate the rate computations, we will form a term-by-emotion rate for
the lexicon:

```{r}
term_scores <- with(affect, unclass(table(term, emotion)))
head(term_scores)
```

Here, `term_scores` is a matrix with entry *(i,j)* indicating the number of
times that term *i* appeared in the affect lexicon with emotion *j*.

We re-classify any term appearing in two or more categories as ambiguous:

```{r}
ncat <- rowSums(term_scores > 0)
term_scores[ncat > 1, c("Positive", "Negative", "Ambiguous")] <- c(0, 0, 1)
```

At this point, every term is in one category, but the score for the term could
be 2, 3, or more, depending on the number of sub-categories the term appeared
in. We replace these larger values with one.

```{r}
term_scores[term_scores > 1] <- 1
```


### Segmenting chapters into smaller chunks

To compute emotion occurrence rates, we start by splitting each chapter into
equal-sized segments of at most 500 tokens. The specific size of 500 tokens is
somewhat arbitrary, but not entirely so. We want the segments to be large enough
so that our rate estimates are reliable, but not so large that the emotion
usage is heterogeneous within the segment.

```{r}
chunks <- text_split(data, "tokens", 500)
```

Within a chapter, the segments all have approximately the same size. However,
since the chapters have different lengths, there is some variation in segment
size across chapters:

```{r}
(n <- text_ntoken(chunks))
```

(If we wanted equal sized segments, we could have concatenated the chapters
together and then split the combined text. The disadvantage of this approach
is that some segments would be split across multiple chapters.) 


### Computing emotion rates

For the count of each emotion category in each segment, we form a text-by-term
matrix of counts, and then multiply this by the term-by-emotion score matrix.

```{r}
x <- term_matrix(chunks, select = rownames(term_scores))
text_scores <- x %*% term_scores
```

For the occurrence rates, we divide the counts by the segment sizes. We then
multiply by 1000 so that rates are given as occurrences per 1000 tokens.

```{r}
# compute the rates per 1000 tokens
unit <- 1000
rate <- list(pos = text_scores[, "Positive"] / n * unit,
             neg = text_scores[, "Negative"] / n * unit,
             ambig = text_scores[, "Ambiguous"] / n * unit)
rate$total <- rate$pos + rate$neg + rate$ambig
```

We use the binomial variance formula to get the standard errors:

```{r}
# compute the standard errors
se <- lapply(rate, function(r) sqrt(r * (unit - r) / n))
```

This is a crude estimate that makes some independence assumptions, but it
gives a reasonable approximation of the uncertainty associated with our
measured rates.



### Plotting the results

We plot the four rate curves as time series. Our main focus is on the total
emotion usage. For this curve, we also put a horizontal dashed line at its
mean, and we indicating the "interesting" segments, those that appear more
than two standard deviations away from the main, by putting error bars on
these points.


```{r emotion, fig.width = 12, fig.cap = "Emotion in Oz"}
# set up segment IDs
i <- seq_len(nrow(chunks))

# set the plot margins, with extra space below the plot
par(mar = c(4, 4, 11, 9) + 0.1, las = 1)

# set up the plot coordinates; put labels but no axes
xlim <- range(i - 0.5, i + 0.5)
ylim <- range(0, rate$total + se$total, rate$total - se$total)
plot(xlim, ylim, type = "n", xlab = "Segment", ylab = "Rate \u00d7 1000", axes = FALSE,
     xaxs = "i")
usr <- par("usr") # get the user coordinates for later

# put tick marks at multiples of 5 on the x axis; labels at multiples of 10
axis(1, at = i[i %% 5 == 0], labels = FALSE)
axis(1, at = i[i %% 10 == 0], labels = TRUE)

# defaults for the y axis
axis(2)

# put vertical lines at chapter boundaries
abline(v = tapply(i, chunks$parent, min) - 0.5, col = "gray")

# put chapter titles above the plot
labels <- data$title
at <- tapply(i, chunks$parent, mean)

# (adapted from https://www.r-bloggers.com/rotated-axis-labels-in-r-plots/)
text(at, usr[4] + 0.01 * diff(usr[3:4]),
     labels = labels, adj = 0, srt = 45, cex = 0.8, xpd = TRUE)

# frame the plot
box()

# colors for the different emotions, from RColorBrewer::brewer.pal(3, "Set2")
col <- c(total = "#000000", pos = "#FC8D62", neg = "#8DA0CB", ambig = "#66C2A5")

# add a legend on the right hand side
legend(usr[2] + 0.015 * diff(usr[1:2]), usr[3] + 0.8 * diff(usr[3:4]),
       legend = c("Total", "Positive", "Negative", "Ambiguous"),
       title = expression(bold("Emotion")),
       fill = col[c("total", "pos", "neg", "ambig")],
       cex = 0.8, xpd = TRUE)

# for the total rate, put a dashed line at the mean rate
abline(h = mean(rate$total), lty = 2, col = col[["total"]])

# plot each rate type
for (t in c("ambig", "neg", "pos", "total")) {
    r <- rate[[t]]
    s <- se[[t]]
    cl <- col[[t]]

    # add lines and points
    lines(i, r, col = cl)
    points(i, r, col = cl, pch = 16, cex = 0.5)

    # for the total, put standard errors around interesting points
    if (t == "total") {
        # "interesting" defined as rate >2 sd away from mean
        int <- abs((r - mean(r)) / sd(r)) > 2

        segments(i[int], (r - s)[int], i[int], (r + s)[int], col = cl)
        segments((i - .2)[int], (r - s)[int], (i + .2)[int], (r - s)[int], col = cl)
        segments((i - .2)[int], (r + s)[int], (i + .2)[int], (r + s)[int], col = cl)
    }
}
```

### Discussion

This is a crude measurement, but it appears to give a reasonable approximation
of the emotional dynamics of the novel.  There are some interesting dynamics
to the "Positive" and "Negative" emotions, but I'm going to focus on the
"Total" emotion.


There are five segments where the rate of emotion word usage is two or more
standard deviations above the mean for the rest of the novel. In all five
cases, these are statistically significant differences (more than two standard
errors above the mean). The first two interesting segments are when Dorothy
meets the Tin Woodman and the Cowardly Lion. The next is when the Dorothy and
her companions meet the Great Oz for the first time and he tasks them with
defeating the Wicked Witch of the West; this is the point in the novel with
the highest emotion word usage. The fourth interesting point is when Oz is
revealed to be a common man, not a great wizard. The last emotional segment is
when Dorothy and her companions leave the Emerald city feeling triumphant and
hopeful.


Summary
-------

The *corpus* library provides facilities for transforming texts into sequences
of tokens and for computing the statistics of these sequences. The
`text_filter()` function allows us to control the transformation from text to
tokens. The `text_stats()` and `term_stats()` functions compute text- and
term-level occurrence statistics. The `text_locate()` function and allow us to
search for terms within texts. The `term_matrix()` function computes a
text-by-term frequency matrix. These functions and their variants provide the
building blocks for analyzing text.

For more information, check the other vignettes or the package documentation
with `library(help = "corpus")`.


[gutenberg-oz]: http://www.gutenberg.org/ebooks/55 "The Wonderful Wizard of Oz"
[heaps-law]: https://en.wikipedia.org/wiki/Heaps%27_law "Heap's law"
[wn-affect]: http://wndomains.fbk.eu/wnaffect.html "WordNet-Affect"
